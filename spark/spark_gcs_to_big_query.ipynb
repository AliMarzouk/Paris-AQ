{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be797342-a380-43eb-af06-1902e51c1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'air-quality-data-0123'\n",
    "JSON_KEY_PATH = '/home/ali_marzouk/air-quality-421919-36f116eb9049.json'\n",
    "BQ_DATASET = 'air_quality_0123'\n",
    "DATE_TABLE_NAME = 'date'\n",
    "LOCATION_TABLE_NAME = 'location'\n",
    "PARAMETER_TABLE_NAME = 'parameter'\n",
    "MEASUREMENT_TABLE_NAME = 'measurement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81b90f48-e8c0-4596-9cb9-eb75f74e201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":/home/ali_marzouk/gcs-connector/gcs-connector-hadoop3-latest.jar:/home/ali_marzouk/bigquery-connector/spark-bigquery-with-dependencies_2.12-0.38.0.jar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['HADOOP_CLASSPATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e7ad5bc-9669-46c9-84f8-74642b72388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = spark = SparkSession.builder \\\n",
    "    .appName('spark_gcs_to_big_query') \\\n",
    "    .config(\"spark.jars\", \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar,https://github.com/GoogleCloudDataproc/spark-bigquery-connector/releases/download/0.38.0/spark-bigquery-with-dependencies_2.12-0.38.0.jar\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") \n",
    "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", JSON_KEY_PATH)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13907454-0ea0-4531-bdb2-ba604d819772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def load_today_data_from_gcs():\n",
    "    today = date.today()\n",
    "    today_date_str = today.strftime(\"%d-%m-%Y\")\n",
    "    return spark.read.format(\"parquet\").load(f\"gs://{BUCKET_NAME}/{today_date_str}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7362572c-b3ff-4d71-ac89-af50c6bbd3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from typing import List\n",
    "\n",
    "def drop_duplicates(dfs: List[SparkDataFrame]):\n",
    "    results = []\n",
    "    for df in dfs:\n",
    "        results.append(df.dropDuplicates([\"external_id\"]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2046902-b3d8-464f-bf17-3505167b8066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, to_timestamp, udf, expr\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def get_tables_dfs(full_data_df):\n",
    "    date_extenal_id_builder_udf = udf(lambda tmpstmp_utc, local: \"{}.{}\".format(tmpstmp_utc, local))\n",
    "    location_external_id_builder_udf = udf(lambda location_id: str(location_id))\n",
    "    measurement_extenal_id_builder_udf = udf(lambda tmpstmp_utc, local, location_id, parameter: \"{}.{}.{}.{}\".format(tmpstmp_utc, local, location_id, parameter))\n",
    "    parameter_externla_id_builder_udf = udf(lambda name, unit: \"{}.{}\".format(name, unit))\n",
    "\n",
    "    read_df = full_data_df.select(explode(\"result.results\").alias('data'))\\\n",
    "            .withColumn('date_uuid', expr(\"uuid()\"))\\\n",
    "            .withColumn('parameter_uuid', expr(\"uuid()\"))\n",
    "    date_df = read_df.select(col(\"date_uuid\").alias('id'), to_timestamp('data.date.utc').alias(\"timestamp_utc\"),'data.date.local', date_extenal_id_builder_udf(\"data.date.utc\", \"data.date.local\").alias(\"external_id\"))\n",
    "    location_df = read_df.select('data.coordinates.longitude', 'data.coordinates.latitude', col(\"data.locationId\").cast(StringType()).alias('id'), \"data.location\", \"data.country\", \"data.city\", location_external_id_builder_udf(\"data.locationId\").alias(\"external_id\"))\n",
    "    parameter_df = read_df.select(col(\"parameter_uuid\").alias('id'), 'data.unit', col('data.parameter').alias('name'), parameter_externla_id_builder_udf(\"data.parameter\", \"data.unit\").alias(\"external_id\"))\n",
    "    measurement_df = read_df \\\n",
    "            .select(\"data.value\", col(\"parameter_uuid\").alias(\"parameter_id\"), col(\"date_uuid\").alias(\"date_id\"), col(\"data.locationId\").alias(\"location_id\").cast(StringType()), \\\n",
    "                    measurement_extenal_id_builder_udf\\\n",
    "                    (to_timestamp('data.date.utc'),\"data.date.local\",col(\"data.locationId\"), \"data.parameter\").alias(\"external_id\"))\n",
    "    return [date_df, location_df, parameter_df, measurement_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2e15958-7bb1-4dbe-b748-d37cf70f628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_bq(table_df, table_name):\n",
    "    table_df.drop('external_id') \\\n",
    "        .write \\\n",
    "        .format(\"bigquery\") \\\n",
    "        .option(\"table\",\"{}.{}\".format(BQ_DATASET, table_name)) \\\n",
    "        .option(\"temporaryGcsBucket\", BUCKET_NAME) \\\n",
    "        .mode('append') \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6e106a7-4cc6-4a53-b274-80c35750a16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spark_gcs_to_big_query] - [SAVING] date data frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spark_gcs_to_big_query] - [SAVED] date data frame\n",
      "[spark_gcs_to_big_query] - [SAVING] location data frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spark_gcs_to_big_query] - [SAVED] location data frame\n",
      "[spark_gcs_to_big_query] - [SAVING] parameter data frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spark_gcs_to_big_query] - [SAVED] parameter data frame\n",
      "[spark_gcs_to_big_query] - [SAVING] measurement data frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spark_gcs_to_big_query] - [SAVED] measurement data frame\n"
     ]
    }
   ],
   "source": [
    "full_data_df = load_today_data_from_gcs()\n",
    "[date_df, location_df, parameter_df, measurement_df] = get_tables_dfs(full_data_df)\n",
    "print (\"[spark_gcs_to_big_query] - [SAVING] date data frame\")\n",
    "save_df_to_bq(date_df, DATE_TABLE_NAME)\n",
    "print (\"[spark_gcs_to_big_query] - [SAVED] date data frame\")\n",
    "\n",
    "print (\"[spark_gcs_to_big_query] - [SAVING] location data frame\")\n",
    "save_df_to_bq(location_df, LOCATION_TABLE_NAME)\n",
    "print (\"[spark_gcs_to_big_query] - [SAVED] location data frame\")\n",
    "\n",
    "print (\"[spark_gcs_to_big_query] - [SAVING] parameter data frame\")\n",
    "save_df_to_bq(parameter_df, PARAMETER_TABLE_NAME)\n",
    "print (\"[spark_gcs_to_big_query] - [SAVED] parameter data frame\")\n",
    "\n",
    "print (\"[spark_gcs_to_big_query] - [SAVING] measurement data frame\")\n",
    "save_df_to_bq(measurement_df, MEASUREMENT_TABLE_NAME)\n",
    "print (\"[spark_gcs_to_big_query] - [SAVED] measurement data frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca650ed4-2fcb-4594-9413-06111cafc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82873842-7a85-4198-b385-991fc6d1b94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
